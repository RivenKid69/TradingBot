# –ò–º—è —Ñ–∞–π–ª–∞: train_model_multi_patch.py
# –ò–ó–ú–ï–ù–ï–ù–ò–Ø (–§–ê–ó–ê 3 - CVaR):
# 1. –î–æ–±–∞–≤–ª–µ–Ω –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä `cvar_alpha` –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ Optuna.
# 2. `cvar_alpha` –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –∞–≥–µ–Ω—Ç–∞ DistributionalPPO.
# –ò–ó–ú–ï–ù–ï–ù–ò–Ø (–§–ê–ó–ê 6 - –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ HPO):
# 1. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π —Å—Ä–µ–¥—ã (env_tr) —Ç–µ–ø–µ—Ä—å
#    —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ —Ñ–∞–π–ª –í–ù–ê–ß–ê–õ–ï –∏—Å–ø—ã—Ç–∞–Ω–∏—è.
# 2. –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Å—Ä–µ–¥–∞ (env_va) –¥–ª—è –ø—Ä—É–Ω–∏–Ω–≥–∞ —Ç–µ–ø–µ—Ä—å —Å–æ–∑–¥–∞–µ—Ç—Å—è
#    –ø—É—Ç–µ–º –ó–ê–ì–†–£–ó–ö–ò —ç—Ç–∏—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫.
# 3. –≠—Ç–æ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç —É—Ç–µ—á–∫—É –¥–∞–Ω–Ω—ã—Ö, –∫–æ–≥–¥–∞ env_va –≤—ã—á–∏—Å–ª—è–ª–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
#    –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏.
# –ò–ó–ú–ï–ù–ï–ù–ò–Ø (–§–ê–ó–ê 7 - –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ HPO –¥–ª—è CVaR):
# 1. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä `cvar_weight` –¥–æ–±–∞–≤–ª–µ–Ω –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ Optuna.
# 2. `cvar_weight` —Ç–µ–ø–µ—Ä—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –º–æ–¥–µ–ª–∏
#    DistributionalPPO, —á—Ç–æ –¥–µ–ª–∞–µ—Ç HPO –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–º.
# –ò–ó–ú–ï–ù–ï–ù–ò–Ø (–û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò):
# 1. –î–æ–±–∞–≤–ª–µ–Ω—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ PyTorch –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ GPU.
# 2. –î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ (`torch.compile`) –¥–ª—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è.
# 3. –¶–∏–∫–ª –æ—Ü–µ–Ω–∫–∏ –≤—ã–Ω–µ—Å–µ–Ω –≤ –±—ã—Å—Ç—Ä—ã–π Cython-–º–æ–¥—É–ª—å.

import os
import glob
import json
import pandas as pd
import numpy as np
from datetime import datetime
import shutil
import random
import math
from features_pipeline import FeaturePipeline
from pathlib import Path

from distributional_ppo import DistributionalPPO
from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor, DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
from stable_baselines3.common.vec_env import VecNormalize
import torch
import optuna
from optuna.samplers import TPESampler
from optuna.pruners import HyperbandPruner
from optuna.exceptions import TrialPruned
from torch.optim.lr_scheduler import OneCycleLR
import multiprocessing as mp
class AdversarialCallback(BaseCallback):
    """
    –ü—Ä–æ–≤–æ–¥–∏—Ç —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç—ã –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ä–µ–∂–∏–º–∞—Ö –∏ –°–û–•–†–ê–ù–Ø–ï–¢
    —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (Sortino Ratio) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∂–∏–º–∞.
    """
    def __init__(self, eval_env: VecEnv, eval_freq: int, regimes: list, regime_duration: int):
        super().__init__()
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.regimes = regimes
        self.regime_duration = regime_duration
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: {'regime_name': sortino_score}
        self.regime_metrics = {}

    def _on_step(self) -> bool:
        if self.n_calls % self.eval_freq == 0:
            print("\n--- Starting Adversarial Regime Stress Tests ---")
            
            for regime in self.regimes:
                print(f"Testing regime: {regime}...")
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∂–∏–º –≤ —Å—Ä–µ–¥–µ
                self.eval_env.env_method("set_market_regime", regime=regime, duration=self.regime_duration)
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É –≤ —ç—Ç–æ–º —Ä–µ–∂–∏–º–µ
                _rewards, equity_curves = evaluate_policy_custom_cython(
                    self.model,
                    self.eval_env,
                    num_episodes=1 # –û–¥–∏–Ω –¥–ª–∏–Ω–Ω—ã–π —ç–ø–∏–∑–æ–¥ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∂–∏–º–∞
                )
                
                # –°—á–∏—Ç–∞–µ–º Sortino –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
                all_returns = [pd.Series(c).pct_change().dropna().to_numpy() for c in equity_curves if len(c) > 1]
                flat_returns = np.concatenate(all_returns) if all_returns else np.array([0.0])
                score = sortino_ratio(flat_returns)
                self.regime_metrics[regime] = score
                
                print(f"Regime '{regime}' | Sortino: {score:.4f}")

            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ä–µ–¥—É –≤ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
            self.eval_env.env_method("set_market_regime", regime='normal', duration=0)
            print("--- Adversarial Tests Finished ---\n")
        return True

    def get_regime_metrics(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–æ–≤."""
        return self.regime_metrics
from shared_memory_vec_env import SharedMemoryVecEnv

# --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –í–∫–ª—é—á–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ PyTorch ---
# –ï—Å–ª–∏ GPU –∏ –≤–µ—Ä—Å–∏—è CUDA >= 11, –≤–∫–ª—é—á–∞–µ–º –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å
if torch.cuda.is_available() and int(torch.version.cuda.split(".")[0]) >= 11:
    torch.set_float32_matmul_precision("high")
# –ü–æ–∑–≤–æ–ª—è–µ—Ç cuDNN –Ω–∞—Ö–æ–¥–∏—Ç—å –ª—É—á—à–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
torch.backends.cudnn.benchmark = True


from trading_patchnew import TradingEnv
from custom_policy_patch import CustomActorCriticPolicy
from fetch_all_data_patch import load_all_data
# --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±—ã—Å—Ç—Ä—É—é Cython-—Ñ—É–Ω–∫—Ü–∏—é –æ—Ü–µ–Ω–∫–∏ ---
from evaluation_utils import evaluate_policy_custom_cython
from data_validation import DataValidator
from utils.model_io import save_sidecar_metadata, check_model_compat
from watchdog_vec_env import WatchdogVecEnv

# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò–ù–î–ò–ö–ê–¢–û–†–û–í (–ï–î–ò–ù–´–ô –ò–°–¢–û–ß–ù–ò–ö –ü–†–ê–í–î–´) ===
MA5_WINDOW = 5
MA20_WINDOW = 20
ATR_WINDOW = 14
RSI_WINDOW = 14
MACD_FAST = 12
MACD_SLOW = 26
MACD_SIGNAL = 9
MOMENTUM_WINDOW = 10
CCI_WINDOW = 14
BB_WINDOW = 20
OBV_MA_WINDOW = 50


class NanGuardCallback(BaseCallback):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç loss –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞ NaN/Inf.
    –ü—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –ø—Ä–µ—Ä—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â–∏–π Optuna-trial.
    """
    def __init__(self, threshold: float = float("inf"), verbose: int = 0):
        super().__init__(verbose)
        self.threshold = threshold       # –ú–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –ª–∏–º–∏—Ç ¬´—Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π¬ª loss

    def _on_rollout_end(self) -> None:
        # 1) –ü—Ä–æ–≤–µ—Ä—è–µ–º loss, –µ—Å–ª–∏ SB3 –ø–æ–ª–æ–∂–∏–ª –µ–≥–æ –≤ –ª–æ–∫–∞–ª—ã
        last_loss = self.locals.get("loss", None)
        if last_loss is not None:
            if (not torch.isfinite(last_loss)) or (torch.abs(last_loss) > self.threshold):
                print("üö®  NaN/Inf –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ loss  ‚Äî  –ø—Ä–µ—Ä—ã–≤–∞–µ–º trial")
                raise TrialPruned("NaN detected in loss")

        # 2) –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        for p in self.model.parameters():
            if p.grad is not None and (not torch.isfinite(p.grad).all()):
                print("üö®  NaN/Inf –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö  ‚Äî  –ø—Ä–µ—Ä—ã–≤–∞–µ–º trial")
                raise TrialPruned("NaN detected in gradients")

    def _on_step(self) -> bool:
        return True

class SortinoPruningCallback(BaseCallback):
    """
    –ö–∞—Å—Ç–æ–º–Ω—ã–π callback –¥–ª—è Optuna, –∫–æ—Ç–æ—Ä—ã–π –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç
    —Ä–µ—à–µ–Ω–∏–µ –æ –ø—Ä—É–Ω–∏–Ω–≥–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ö–û–≠–§–§–ò–¶–ò–ï–ù–¢–ê –°–û–†–¢–ò–ù–û, –∞ –Ω–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.
    """
    def __init__(self, trial: optuna.Trial, eval_env: VecEnv, n_eval_episodes: int = 5, eval_freq: int = 10000, verbose: int = 0):
        super().__init__(verbose)
        self.trial = trial
        self.eval_env = eval_env
        self.n_eval_episodes = n_eval_episodes
        self.eval_freq = eval_freq

    def _on_step(self) -> bool:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É –≤ –∑–∞–¥–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—É—é Cython-—Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            rewards, equity_curves = evaluate_policy_custom_cython(
                self.model, 
                self.eval_env, 
                num_episodes=self.n_eval_episodes
            )
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Sortino –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∫—Ä–∏–≤—ã—Ö –∫–∞–ø–∏—Ç–∞–ª–∞
            if not equity_curves:
                # –ï—Å–ª–∏ –ø–æ –∫–∞–∫–æ–π-—Ç–æ –ø—Ä–∏—á–∏–Ω–µ –æ—Ü–µ–Ω–∫–∞ –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å—á–∏—Ç–∞–µ–º Sortino —Ä–∞–≤–Ω—ã–º 0
                current_sortino = 0.0
            else:
                all_returns = [
                    pd.Series(curve).pct_change().dropna().to_numpy() 
                    for curve in equity_curves if len(curve) > 1
                ]
                flat_returns = np.concatenate(all_returns) if all_returns else np.array([0.0])
                current_sortino = sortino_ratio(flat_returns)

            if self.verbose > 0:
                print(f"Step {self.n_calls}: Pruning check with Sortino Ratio = {current_sortino:.4f}")

            # 1. –°–æ–æ–±—â–∞–µ–º Optuna –æ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ (—Ç–µ–ø–µ—Ä—å —ç—Ç–æ Sortino)
            self.trial.report(current_sortino, self.n_calls)

            # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω—É–∂–Ω–æ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —ç—Ç–æ—Ç trial
            if self.trial.should_prune():
                raise TrialPruned(f"Trial pruned at step {self.n_calls} with Sortino Ratio: {current_sortino:.4f}")

        return True

class ObjectiveScorePruningCallback(BaseCallback):
    """
    Callback –¥–ª—è –ø—Ä—É–Ω–∏–Ω–≥–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø–æ–ª–Ω—É—é –≤–∑–≤–µ—à–µ–Ω–Ω—É—é –º–µ—Ç—Ä–∏–∫—É objective_score.
    –†–∞–±–æ—Ç–∞–µ—Ç —Ä–µ–∂–µ, —á–µ–º SortinoPruningCallback, —Ç–∞–∫ –∫–∞–∫ –æ—Ü–µ–Ω–∫–∞ –∑–∞–Ω–∏–º–∞–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏.
    """
    def __init__(self, trial: optuna.Trial, eval_env: VecEnv, eval_freq: int = 40000, verbose: int = 0):
        super().__init__(verbose)
        self.trial = trial
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        
        # –í–µ—Å–∞, –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–µ
        self.main_weight = 0.5
        self.choppy_weight = 0.3
        self.trend_weight = 0.2
        self.regime_duration = 2_500

    def _on_step(self) -> bool:
        if self.eval_freq > 0 and self.n_calls > 0 and self.n_calls % self.eval_freq == 0:
            
            print(f"\n--- Step {self.n_calls}: Starting comprehensive pruning check with Objective Score ---")
            
            regimes_to_evaluate = ['normal', 'choppy_flat', 'strong_trend']
            evaluated_metrics = {}

            try:
                for regime in regimes_to_evaluate:
                    if self.verbose > 0:
                        print(f"Pruning evaluation: testing regime '{regime}'...")

                    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞–¥–≤–µ—Ä—Å–∞—Ä–∏–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
                    if regime != 'normal':
                        self.eval_env.env_method("set_market_regime", regime=regime, duration=self.regime_duration)

                    # –î–ª—è –ø—Ä—É–Ω–∏–Ω–≥–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ–Ω—å—à–µ–≥–æ —á–∏—Å–ª–∞ —ç–ø–∏–∑–æ–¥–æ–≤, —á–µ–º –≤ —Ñ–∏–Ω–∞–ª–µ
                    num_episodes = 5  # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 5 —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
                    
                    _rewards, equity_curves = evaluate_policy_custom_cython(
                        self.model, self.eval_env, num_episodes=num_episodes
                    )
                    
                    all_returns = [pd.Series(c).pct_change().dropna().to_numpy() for c in equity_curves if len(c) > 1]
                    flat_returns = np.concatenate(all_returns) if all_returns else np.array([0.0])
                    score = sortino_ratio(flat_returns)
                    evaluated_metrics[regime] = score

            finally:
                # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –≤—Å–µ–≥–¥–∞ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ä–µ–¥—É –≤ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º,
                # —á—Ç–æ–±—ã –Ω–µ –≤–ª–∏—è—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –¥—Ä—É–≥–∏–µ –∫–æ–ª–±—ç–∫–∏.
                self.eval_env.env_method("set_market_regime", regime='normal', duration=0)

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—É—é –º–µ—Ç—Ä–∏–∫—É
            main_sortino = evaluated_metrics.get('normal', -1.0)
            choppy_score = evaluated_metrics.get('choppy_flat', -1.0)
            trend_score = evaluated_metrics.get('strong_trend', -1.0)
            
            objective_score = (self.main_weight * main_sortino + 
                               self.choppy_weight * choppy_score + 
                               self.trend_weight * trend_score)

            if self.verbose > 0:
                print(f"Comprehensive pruning check complete. Objective Score: {objective_score:.4f}")
                print(f"Components -> Main: {main_sortino:.4f}, Choppy: {choppy_score:.4f}, Trend: {trend_score:.4f}\n")

            # –°–æ–æ–±—â–∞–µ–º Optuna –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä—É–Ω–∏–Ω–≥–∞
            self.trial.report(objective_score, self.n_calls)
            if self.trial.should_prune():
                raise TrialPruned(f"Trial pruned at step {self.n_calls} with Objective Score: {objective_score:.4f}")

        return True

def sharpe_ratio(returns: np.ndarray, risk_free_rate: float = 0.0) -> float:
    std = np.std(returns)
    return np.mean(returns - risk_free_rate) / (std + 1e-9) * np.sqrt(365 * 24)

def sortino_ratio(returns: np.ndarray, risk_free_rate: float = 0.0) -> float:
¬† ¬† downside = returns[returns < risk_free_rate] - risk_free_rate
¬† ¬† if downside.size == 0:
        # –ï—Å–ª–∏ –Ω–µ—Ç —É–±—ã—Ç–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ (–∫–∞–∫ –≤ –®–∞—Ä–ø–µ).
        # –≠—Ç–æ –±–æ–ª–µ–µ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∏—Å–∫, —á–µ–º –≤–æ–∑–≤—Ä–∞—Ç –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã.
        std = np.std(returns)
        # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –Ω–æ–ª—å, –µ—Å–ª–∏ –≤—Å–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã.
        if std < 1e-9:
            return 0.0
        return np.mean(returns - risk_free_rate) / std * np.sqrt(365 * 24)

¬† ¬† downside_std = np.sqrt(np.mean(downside**2)) + 1e-9
    # –≠—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∏–∑–±—ã—Ç–æ—á–Ω–æ–π, –µ—Å–ª–∏ downside_std –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑, 
    # –Ω–æ –æ—Å—Ç–∞–≤–∏–º –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.
    return np.mean(returns - risk_free_rate) / downside_std * np.sqrt(365 * 24)

# --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –°—Ç–∞—Ä–∞—è Python-—Ñ—É–Ω–∫—Ü–∏—è —É–¥–∞–ª–µ–Ω–∞, —Ç–∞–∫ –∫–∞–∫ –∑–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ Cython-–≤–µ—Ä—Å–∏—é ---

def objective(trial: optuna.Trial,
              total_timesteps: int,
              train_data_by_token: dict,
              train_obs_by_token: dict,
              val_data_by_token: dict,
              val_obs_by_token: dict,
              norm_stats: dict):

    print(f">>> Trial {trial.number+1} with budget={total_timesteps}")
    

    # –ò–°–ü–†–ê–í–õ–ï–ù–û: window_size –≤–æ–∑–≤—Ä–∞—â–µ–Ω –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ HPO
    params = {
        "window_size": trial.suggest_categorical("window_size", [10, 20, 30]),
        "trade_frequency_penalty": trial.suggest_float("trade_frequency_penalty", 1e-5, 5e-4, log=True),
        "n_steps": trial.suggest_categorical("n_steps", [512, 1024, 2048]),
        "n_epochs": trial.suggest_int("n_epochs", 1, 5),
        "batch_size": trial.suggest_categorical("batch_size", [64, 128, 256]),
        "ent_coef": trial.suggest_float("ent_coef", 5e-5, 5e-3, log=True),
        "learning_rate": trial.suggest_float("learning_rate", 1e-4, 5e-4, log=True),
        "risk_aversion_drawdown": trial.suggest_float("risk_aversion_drawdown", 0.05, 0.3),
        "risk_aversion_variance": trial.suggest_float("risk_aversion_variance", 0.005, 0.01),
        "weight_decay": trial.suggest_float("weight_decay", 1e-6, 1e-2, log=True),
        "gamma": trial.suggest_float("gamma", 0.97, 0.995),
        "gae_lambda": trial.suggest_float("gae_lambda", 0.8, 1.0),
        "clip_range": trial.suggest_float("clip_range", 0.12, 0.18),
        "max_grad_norm": trial.suggest_float("max_grad_norm", 0.3, 1.0),
        "hidden_dim": trial.suggest_categorical("hidden_dim", [64, 128, 256]),
        "atr_multiplier": trial.suggest_float("atr_multiplier", 1.5, 3.0),
        "trailing_atr_mult": trial.suggest_float("trailing_atr_mult", 1.0, 2.0),
        "tp_atr_mult": trial.suggest_float("tp_atr_mult", 2.0, 4.0),
        "cql_alpha": trial.suggest_float("cql_alpha", 0.1, 10.0, log=True),
        "cql_beta": trial.suggest_float("cql_beta", 1.0, 10.0),
        "cvar_alpha": trial.suggest_float("cvar_alpha", 0.01, 0.20),
        "momentum_factor": trial.suggest_float("momentum_factor", 0.1, 0.7),
        "mean_reversion_factor": trial.suggest_float("mean_reversion_factor", 0.2, 0.8),
        "adversarial_factor": trial.suggest_float("adversarial_factor", 0.3, 0.9),
        "cvar_weight": trial.suggest_float("cvar_weight", 0.1, 2.0, log=True),
        "vf_coef": trial.suggest_float("vf_coef", 0.05, 0.5, log=True), # <-- –î–û–ë–ê–í–õ–ï–ù–û
        "v_range_ema_alpha": trial.suggest_float("v_range_ema_alpha", 0.005, 0.05, log=True),
    }
    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∫–Ω–æ —Å–∞–º–æ–≥–æ "–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ" –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    #    –≠—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤ C++ —Å–∏–º—É–ª—è—Ç–æ—Ä.
    #    (–í –¥–∞–Ω–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –æ–Ω–∏ –∂–µ—Å—Ç–∫–æ –∑–∞–¥–∞–Ω—ã –≤ TradingEnv, –Ω–æ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –±–µ—Ä–µ–º –∏—Ö –∏–∑ HPO)
    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∫–Ω–æ —Å–∞–º–æ–≥–æ "–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ" –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç.
    #    –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Å—Ä–µ–¥—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
    slowest_window = max(
        params["window_size"],
        MA20_WINDOW,
        MACD_SLOW,
        ATR_WINDOW,
        RSI_WINDOW,
        CCI_WINDOW,
        BB_WINDOW,
        OBV_MA_WINDOW
    )
    
    # 2. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–∏–æ–¥ –ø—Ä–æ–≥—Ä–µ–≤–∞ —Å –∑–∞–ø–∞—Å–æ–º (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è x2).
    #    –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—É—é —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—é –¥–∞–∂–µ –¥–ª—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–π,
    #    –∫–∞–∫ –≤ —Å–∏–≥–Ω–∞–ª—å–Ω–æ–π –ª–∏–Ω–∏–∏ MACD.
    warmup_period = slowest_window * 2

    policy_arch_params = {
        "hidden_dim": params["hidden_dim"],
        "use_memory": True,
        "num_atoms": 51,  
    }

    if not train_data_by_token: raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ —ç—Ç–æ–º trial.")

    n_envs = 8
    print(f"–ó–∞–ø—É—Å–∫–∞–µ–º {n_envs} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥...")
    def make_env_train(rank: int):
        def _init():
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π seed –¥–ª—è –∫–∞–∂–¥–æ–≥–æ trial-–∞ –∏ –∫–∞–∂–¥–æ–≥–æ –≤–æ—Ä–∫–µ—Ä–∞
            # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å –∏ –¥–µ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é
            unique_seed = trial.number * 100 + rank 
            env_params = {
                # 1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥–±–∏—Ä–∞–µ—Ç Optuna
                "risk_aversion_drawdown": params["risk_aversion_drawdown"],
                "risk_aversion_variance": params["risk_aversion_variance"],
                "atr_multiplier": params["atr_multiplier"],
                "trailing_atr_mult": params["trailing_atr_mult"],
                "tp_atr_mult": params["tp_atr_mult"],
                "window_size": params.get("window_size", 20),
                "gamma": params["gamma"],
                "trade_frequency_penalty": params["trade_frequency_penalty"],
                "momentum_factor": params["momentum_factor"],
                "mean_reversion_factor": params["mean_reversion_factor"],
                "adversarial_factor": params["adversarial_factor"],

                # 2. –î–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤ —Ñ—É–Ω–∫—Ü–∏—é objective
                "df_dict": train_data_by_token,
                "obs_dict": train_obs_by_token,
                "norm_stats": norm_stats,

                # 3. –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
                "mode": "train",
                "reward_shaping": True,
                "warmup_period": warmup_period,
                "ma5_window": MA5_WINDOW,
                "ma20_window": MA20_WINDOW,
                "atr_window": ATR_WINDOW,
                "rsi_window": RSI_WINDOW,
                "macd_fast": MACD_FAST,
                "macd_slow": MACD_SLOW,
                "macd_signal": MACD_SIGNAL,
                "momentum_window": MOMENTUM_WINDOW,
                "cci_window": CCI_WINDOW,
                "bb_window": BB_WINDOW,
                "obv_ma_window": OBV_MA_WINDOW,
                 
            }
            
            # –°–æ–∑–¥–∞–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Å—Ä–µ–¥—ã
            return TradingEnv(**env_params)
        return _init

    env_constructors = [make_env_train(rank=i) for i in range(n_envs)]

    stats_path = f"models/trials/vec_normalize_{trial.number}.pkl"
    os.makedirs(os.path.dirname(stats_path), exist_ok=True)

    base_env_tr = WatchdogVecEnv(env_constructors)
    monitored_env_tr = VecMonitor(base_env_tr)
    env_tr = VecNormalize(monitored_env_tr, norm_obs=False, norm_reward=True, clip_reward=10.0, gamma=params["gamma"])

    

    env_tr.save(stats_path)
    save_sidecar_metadata(stats_path, extra={"kind": "vecnorm_stats"})

    def make_env_val():
        env_val_params = {
            "df_dict": val_data_by_token,
            "obs_dict": val_obs_by_token,
            "norm_stats": norm_stats,
            "window_size": params["window_size"],
            "gamma": params["gamma"],
            "atr_multiplier": params["atr_multiplier"],
            "trailing_atr_mult": params["trailing_atr_mult"],
            "tp_atr_mult": params["tp_atr_mult"],
            "trade_frequency_penalty": params["trade_frequency_penalty"],
            "mode": "val",
            "reward_shaping": False,
            "warmup_period": warmup_period,
            "ma5_window": MA5_WINDOW,
            "ma20_window": MA20_WINDOW,
            "atr_window": ATR_WINDOW,
            "rsi_window": RSI_WINDOW,
            "macd_fast": MACD_FAST,
            "macd_slow": MACD_SLOW,
            "macd_signal": MACD_SIGNAL,
            "momentum_window": MOMENTUM_WINDOW,
            "cci_window": CCI_WINDOW,
            "bb_window": BB_WINDOW,
            "obv_ma_window": OBV_MA_WINDOW
        }
        return TradingEnv(**env_val_params)

    monitored_env_va = VecMonitor(DummyVecEnv([make_env_val]))
    check_model_compat(stats_path)
    env_va = VecNormalize.load(stats_path, monitored_env_va)
    env_va.training = False
    env_va.norm_reward = False
    env_va.training = False
    env_va.norm_reward = False

    policy_kwargs = {
        "arch_params": policy_arch_params,
        "optimizer_class": torch.optim.AdamW,
        "optimizer_kwargs": {"weight_decay": params["weight_decay"]},
    }
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –±—É–¥–µ—Ç —Å–æ–±—Ä–∞–Ω –ø–æ–ª–Ω—ã–π –±—É—Ñ–µ—Ä –¥–∞–Ω–Ω—ã—Ö (rollout)
    num_rollouts = math.ceil(total_timesteps / (params["n_steps"] * n_envs))
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º, –Ω–∞ —Å–∫–æ–ª—å–∫–æ –º–∏–Ω–∏-–±–∞—Ç—á–µ–π –¥–µ–ª–∏—Ç—Å—è –∫–∞–∂–¥—ã–π —Ä–æ–ª–ª–∞—É—Ç
    num_minibatches_per_rollout = (params["n_steps"] * n_envs) // params["batch_size"]
    
    # –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∑–∞ –≤—Å–µ –æ–±—É—á–µ–Ω–∏–µ
    total_optimizer_steps = num_rollouts * params["n_epochs"] * num_minibatches_per_rollout
    
    # –°–æ–∑–¥–∞–µ–º lambda-—Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
    # SB3 –≤—ã–∑–æ–≤–µ—Ç –µ–µ —Å–æ —Å–≤–æ–∏–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º, –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º
    def scheduler_fn(optimizer):
        return OneCycleLR(optimizer=optimizer, max_lr=params["learning_rate"] * 3, total_steps=total_optimizer_steps)
    
    # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –µ–µ –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ policy_kwargs
    policy_kwargs["lr_scheduler"] = scheduler_fn
    model = DistributionalPPO(
        use_torch_compile=True,
        v_range_ema_alpha=params["v_range_ema_alpha"],
        policy=CustomActorCriticPolicy,
        env=env_tr,
        cql_alpha=params["cql_alpha"],
        cql_beta=params["cql_beta"],
        cvar_alpha=params["cvar_alpha"],
        vf_coef=params["vf_coef"],
        cvar_weight=params["cvar_weight"],
        
        learning_rate=params["learning_rate"],
        n_steps=params["n_steps"],
        n_epochs=params["n_epochs"],
        batch_size=params["batch_size"],
        ent_coef=params["ent_coef"],
        gamma=params["gamma"],
        gae_lambda=params["gae_lambda"],
        clip_range=params["clip_range"],
        max_grad_norm=params["max_grad_norm"],
        policy_kwargs=policy_kwargs,
        tensorboard_log="tensorboard_logs",
        verbose=1
    )

    



    nan_guard = NanGuardCallback()

    # –ë—ã—Å—Ç—Ä—ã–π –∫–æ–ª–±—ç–∫ –¥–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –æ—Ç—Å–µ—á–µ–Ω–∏—è –ø–æ –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç—Ä–∏–∫–µ
    sortino_pruner = SortinoPruningCallback(trial, eval_env=env_va, eval_freq=8_000, n_eval_episodes=10)

    # –ú–µ–¥–ª–µ–Ω–Ω—ã–π, –Ω–æ —Ç–æ—á–Ω—ã–π –∫–æ–ª–±—ç–∫ –¥–ª—è –ø–æ–∑–¥–Ω–µ–≥–æ –æ—Ç—Å–µ—á–µ–Ω–∏—è –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ
    objective_pruner = ObjectiveScorePruningCallback(trial, eval_env=env_va, eval_freq=40_000, verbose=1)

    all_callbacks = [nan_guard, sortino_pruner, objective_pruner]

    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=all_callbacks,
            progress_bar=True
        )
    finally:
        env_tr.close()
        env_va.close()

    trial_model_path = f"models/trials/trial_{trial.number}_model.zip"
    model.save(trial_model_path)
    save_sidecar_metadata(trial_model_path, extra={"kind": "sb3_model", "trial": int(trial.number)})

    

    print(f"<<< Trial {trial.number+1} finished training, starting unified final evaluation‚Ä¶")

    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Å–µ —Ä–µ–∂–∏–º—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    regimes_to_evaluate = ['normal', 'choppy_flat', 'strong_trend']
    final_metrics = {}
    regime_duration = 2_500 

    # 2. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∫–∞–∂–¥–æ–º —Ä–µ–∂–∏–º–µ
    for regime in regimes_to_evaluate:
        def make_final_eval_env():
            final_env_params = {
                "df_dict": val_data_by_token, "obs_dict": val_obs_by_token,
                "norm_stats": norm_stats, "window_size": params["window_size"],
                "gamma": params["gamma"], "atr_multiplier": params["atr_multiplier"],
                "trailing_atr_mult": params["trailing_atr_mult"], "tp_atr_mult": params["tp_atr_mult"],
                "trade_frequency_penalty": params["trade_frequency_penalty"], "mode": "val",
                "reward_shaping": False, "warmup_period": warmup_period,
                "ma5_window": MA5_WINDOW, "ma20_window": MA20_WINDOW, "atr_window": ATR_WINDOW,
                "rsi_window": RSI_WINDOW, "macd_fast": MACD_FAST, "macd_slow": MACD_SLOW,
                "macd_signal": MACD_SIGNAL, "momentum_window": MOMENTUM_WINDOW,
                "cci_window": CCI_WINDOW, "bb_window": BB_WINDOW, "obv_ma_window": OBV_MA_WINDOW
            }
            return TradingEnv(**final_env_params)

        check_model_compat(stats_path)
        final_eval_env = VecMonitor(
            VecNormalize.load(stats_path, DummyVecEnv([make_final_eval_env]))
        )
        final_eval_env.training = False
        final_eval_env.norm_reward = False

        if regime != 'normal':
            final_eval_env.env_method("set_market_regime", regime=regime, duration=regime_duration)

        num_episodes = len(val_data_by_token.keys())
        _rewards, equity_curves = evaluate_policy_custom_cython(model, final_eval_env, num_episodes=num_episodes)

        all_returns = [pd.Series(c).pct_change().dropna().to_numpy() for c in equity_curves if len(c) > 1]
        flat_returns = np.concatenate(all_returns) if all_returns else np.array([0.0])
        final_metrics[regime] = sortino_ratio(flat_returns)

        nt_stats = final_eval_env.env_method("get_no_trade_stats")
        total_steps = sum(s["total_steps"] for s in nt_stats if s)
        blocked_steps = sum(s["blocked_steps"] for s in nt_stats if s)
        ratio = blocked_steps / total_steps if total_steps else 0.0
        print(
            f"No-trade blocks: {blocked_steps}/{total_steps} steps ({ratio:.2%})"
        )

        final_eval_env.close()

    # --- –†–ê–°–ß–ï–¢ –ò–¢–û–ì–û–í–û–ô –í–ó–í–ï–®–ï–ù–ù–û–ô –ú–ï–¢–†–ò–ö–ò ---
    main_sortino = final_metrics.get('normal', -1.0)
    choppy_score = final_metrics.get('choppy_flat', -1.0)
    trend_score = final_metrics.get('strong_trend', -1.0)

    main_weight = 0.5
    choppy_weight = 0.3
    trend_weight = 0.2
 
    objective_score = (main_weight * main_sortino + choppy_weight * choppy_score + trend_weight * trend_score)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    trial.set_user_attr("main_sortino", main_sortino)
    trial.set_user_attr("choppy_sortino", choppy_score)
    trial.set_user_attr("trend_sortino", trend_score)
    trial.set_user_attr("final_objective", objective_score)

    
    trial.set_user_attr("final_return", 0.0) # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤ 0, —Ç.–∫. –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä–∞—Å—á–µ—Ç —É—Å–ª–æ–∂–Ω–µ–Ω

    print(f"\n[‚úÖ Trial {trial.number}] COMPLETE. Final Weighted Score: {objective_score:.4f}")
    print(f"   Components -> Main Sortino: {main_sortino:.4f}, Choppy: {choppy_score:.4f}, Trend: {trend_score:.4f}\n")

    return objective_score

def main():
    PROCESSED_DATA_DIR = "data/processed"
    N_ENSEMBLE = 5
    # --- –î–æ–±–∞–≤–ª–µ–Ω–Ω—ã–π –±–ª–æ–∫ –æ—á–∏—Å—Ç–∫–∏ ---
    trials_dir = "models/trials"
    if os.path.exists(trials_dir):
        print(f"–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ '{trials_dir}'...")
        shutil.rmtree(trials_dir)
    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    os.makedirs(trials_dir, exist_ok=True)

    print("Loading all pre-processed data...")
    all_feather_files = glob.glob(os.path.join(PROCESSED_DATA_DIR, "*.feather"))
    if not all_feather_files:
        raise FileNotFoundError(
            f"No .feather files found in {PROCESSED_DATA_DIR}. "
            f"Run prepare_advanced_data.py (Fear&Greed), prepare_events.py (macro events), "
            f"incremental_klines.py (1h candles), then prepare_and_run.py (merge/export)."
        )

    all_dfs_dict, all_obs_dict = load_all_data(all_feather_files, synthetic_fraction=0, seed=42)

    # Fit/Save pipeline on full training data and add standardized features (_z)
    PREPROC_PATH = Path("models") / "preproc_pipeline.json"
    pipe = FeaturePipeline()
    pipe.fit(all_dfs_dict)
    PREPROC_PATH.parent.mkdir(parents=True, exist_ok=True)
    pipe.save(str(PREPROC_PATH))
    all_dfs_dict = pipe.transform_dict(all_dfs_dict, add_suffix="_z")
    print(f"Feature pipeline fitted and saved to {PREPROC_PATH}. Standardized columns *_z added.")
    print("To run inference over processed data, execute: python infer_signals.py")
    # --- –ì–µ–π—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: —Å—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è OHLCV –ø–µ—Ä–µ–¥ —Å–ø–ª–∏—Ç–æ–º ---
    _validator = DataValidator()
    for _key, _df in all_dfs_dict.items():
        try:
            # frequency=None -> –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞
            _validator.validate(_df, frequency=None)
        except Exception as e:
            raise RuntimeError(f"Data validation failed for asset '{_key}': {e}")
    print("‚úì Data validation passed for all assets.")

    
    
    print("Splitting data into train/validation sets...")

    full_train_data, full_val_data = {}, {}
    full_train_obs, full_val_obs = {}, {}

    rng = random.Random(42)
    all_keys = list(all_dfs_dict.keys())
    rng.shuffle(all_keys)

    split_idx = int(0.8 * len(all_keys))
    train_keys = all_keys[:split_idx]
    val_keys   = all_keys[split_idx:]

    for key in train_keys:
        full_train_data[key] = all_dfs_dict[key]
        if key in all_obs_dict:
            full_train_obs[key] = all_obs_dict[key]

    for key in val_keys:
        full_val_data[key] = all_dfs_dict[key]
        if key in all_obs_dict:
            full_val_obs[key] = all_obs_dict[key]


    print(f"Data split: {len(full_train_data)} assets for training, {len(full_val_data)} for validation.")

    print("Calculating per-asset normalization stats from the training set...")
    norm_stats = {}
    
    # –ò—Ç–µ—Ä–∏—Ä—É–µ–º –ø–æ –∫–∞–∂–¥–æ–º—É –∞–∫—Ç–∏–≤—É –≤ –¢–†–ï–ù–ò–†–û–í–û–ß–ù–û–ú –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö
    for asset_key, train_df in full_train_data.items():
        
        # 1. –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –î–ê–ù–ù–û–ú –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –∞–∫—Ç–∏–≤–µ
        features_to_normalize = [
            col for col in train_df.columns 
            if '_norm' in col and col not in ['log_volume_norm', 'fear_greed_value_norm']
        ]
        
        if not features_to_normalize:
            continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ —É —ç—Ç–æ–≥–æ –∞—Å—Å–µ—Ç–∞ –Ω–µ—Ç —Ç–∞–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            
        # 2. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¢–û–õ–¨–ö–û –ø–æ –¥–∞–Ω–Ω—ã–º —ç—Ç–æ–≥–æ –∞—Å—Å–µ—Ç–∞
        mean_stats = train_df[features_to_normalize].mean().to_dict()
        std_stats = train_df[features_to_normalize].std().to_dict()
        
        # 3. –ù–∞—Ö–æ–¥–∏–º ID —Ç–æ–∫–µ–Ω–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å —ç—Ç–∏–º –∞—Å—Å–µ—Ç–æ–º
        # (–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –æ–¥–∏–Ω —Ñ–∞–π–ª = –æ–¥–∏–Ω –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–æ–∫–µ–Ω)
        if 'token_id' in train_df.columns:
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
            if not train_df.empty:
                token_id = train_df['token_id'].iloc[0]
                
                # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
                norm_stats[str(token_id)] = {'mean': mean_stats, 'std': std_stats}

    with open("models/norm_stats.json", "w") as f:
        json.dump(norm_stats, f, indent=4)
    print(f"Per-asset normalization stats for {len(norm_stats)} tokens calculated and saved.")

    HPO_TRIALS = 20 # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø—ã—Ç–∞–Ω–∏–π
    HPO_BUDGET_PER_TRIAL = 1_000_000 # –¢–∞–π–º—Å—Ç–µ–ø—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å–ø—ã—Ç–∞–Ω–∏—è

    print(f"\n===== Starting Unified HPO Process ({HPO_TRIALS} trials) =====")

    sampler = TPESampler(n_startup_trials=5, seed=42)
    pruner = HyperbandPruner()
    study = optuna.create_study(direction="maximize", sampler=sampler, pruner=pruner)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞ –ü–û–õ–ù–û–ú, –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö
    study.optimize(
        lambda t: objective(t, HPO_BUDGET_PER_TRIAL,
                            full_train_data, full_train_obs,
                            full_val_data, full_val_obs,
                            norm_stats),
        n_trials=HPO_TRIALS,
        n_jobs=1
    )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
    final_study = study
    if not final_study: print("No final study completed. Exiting."); return
    # <-- –ö–û–ù–ï–¶ –ë–õ–û–ö–ê –î–õ–Ø –ó–ê–ú–ï–ù–´ -->

    print(f"\nSaving best {N_ENSEMBLE} models from the final stage...")
    ensemble_dir = "models/ensemble"
    if os.path.exists(ensemble_dir): shutil.rmtree(ensemble_dir)
    os.makedirs(ensemble_dir)

    top_trials = sorted(final_study.trials, key=lambda tr: tr.value or -1e9, reverse=True)[:N_ENSEMBLE]

    ensemble_meta = []
    for i, trial in enumerate(top_trials):
        model_idx = i + 1
        src_model = f"models/trials/trial_{trial.number}_model.zip"
        src_stats = f"models/trials/vec_normalize_{trial.number}.pkl"

        if os.path.exists(src_model):
            shutil.copyfile(src_model, os.path.join(ensemble_dir, f"model_{model_idx}.zip"))
            if os.path.exists(src_stats):
                shutil.copyfile(src_stats, os.path.join(ensemble_dir, f"vec_normalize_{model_idx}.pkl"))

            ensemble_meta.append({"ensemble_index": model_idx, "trial_number": trial.number, "value": trial.value, "params": trial.params})
        else:
            print(f"‚ö†Ô∏è WARNING: Could not find model for trial {trial.number}. Skipping.")
    # –ö–æ–ø–∏—Ä—É–µ–º –µ–¥–∏–Ω—ã–π —Ñ–∞–π–ª —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞–º–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π,
    # —Ç–∞–∫ –∫–∞–∫ –æ–Ω —è–≤–ª—è–µ—Ç—Å—è –Ω–µ–æ—Ç—ä–µ–º–ª–µ–º–æ–π —á–∞—Å—Ç—å—é –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ.
    src_norm_stats = "models/norm_stats.json"
    if os.path.exists(src_norm_stats):
        shutil.copyfile(src_norm_stats, os.path.join(ensemble_dir, "norm_stats.json"))
    else:
        print(f"‚ö†Ô∏è CRITICAL WARNING: Could not find the global 'norm_stats.json' file. The saved ensemble will not be usable for inference.")
    with open(os.path.join(ensemble_dir, "ensemble_meta.json"), "w") as f: json.dump(ensemble_meta, f, indent=4)
    print(f"\n‚úÖ Ensemble of {len(ensemble_meta)} models saved to '{ensemble_dir}'. HPO complete.")

if __name__ == "__main__":
    try:
        mp.set_start_method("spawn", force=True)
    except RuntimeError:
        pass

    # --- gradient sanity check (–≤–∫–ª—é—á–∞–µ—Ç—Å—è —Ñ–ª–∞–≥–æ–º –æ–∫—Ä—É–∂–µ–Ω–∏—è) ---—á—Ç–æ 
    from runtime_flags import get_bool
    if get_bool("GRAD_SANITY", False):
        from tools.grad_sanity import run_check
        run_check()

    main()